{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NicholasMcFadden/NLP-Text_Mining/blob/main/All_Senti_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aih0CPWZv76A"
      },
      "source": [
        "# Step 1: Read in data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "id": "Sgth6EjHwWdD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24245d45-7aeb-448b-8fb3-f83988e54eda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "FyS-umC8v76C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c14a8e7-963b-4f45-f4b6-b3fb1a3ba448"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                              Review  Rating\n",
            "0  nice hotel expensive parking got good deal sta...       4\n",
            "1  ok nothing special charge diamond member hilto...       2\n",
            "2  nice rooms not 4* experience hotel monaco seat...       3\n",
            "3  unique, great stay, wonderful time hotel monac...       5\n",
            "4  great stay great stay, went seahawk game aweso...       5\n",
            "5  love monaco staff husband stayed hotel crazy w...       5\n",
            "6  cozy stay rainy city, husband spent 7 nights m...       5\n",
            "7  excellent staff, housekeeping quality hotel ch...       4\n",
            "8  hotel stayed hotel monaco cruise, rooms genero...       5\n",
            "9  excellent stayed hotel monaco past w/e delight...       5\n",
            "20491\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# NLTK\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
        "\n",
        "# Gensim libraries\n",
        "import gensim\n",
        "import gensim.corpora as corpora\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.models.phrases import Phrases, Phraser\n",
        "\n",
        "# SpaCy\n",
        "import spacy\n",
        "\n",
        "# Sci-Kit\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Imblearn\n",
        "\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "\n",
        "\n",
        "train = pd.read_csv(\"drive/Shareddrives/IST_736_Final_Project/tripadvisor_hotel_reviews.csv\")\n",
        "\n",
        "\n",
        "print(train[:10])\n",
        "y=train['Rating'].values\n",
        "X=train['Review'].values\n",
        "\n",
        "\n",
        "og_len = len(X)\n",
        "print(og_len)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X[:2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gjjoPjD2q4dz",
        "outputId": "5f341da3-e559-4309-8b70-9687cd5e4fc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['nice hotel expensive parking got good deal stay hotel anniversary, arrived late evening took advice previous reviews did valet parking, check quick easy, little disappointed non-existent view room room clean nice size, bed comfortable woke stiff neck high pillows, not soundproof like heard music room night morning loud bangs doors opening closing hear people talking hallway, maybe just noisy neighbors, aveda bath products nice, did not goldfish stay nice touch taken advantage staying longer, location great walking distance shopping, overall nice experience having pay 40 parking night,  ',\n",
              "       'ok nothing special charge diamond member hilton decided chain shot 20th anniversary seattle, start booked suite paid extra website description not, suite bedroom bathroom standard hotel room, took printed reservation desk showed said things like tv couch ect desk clerk told oh mixed suites description kimpton website sorry free breakfast, got kidding, embassy suits sitting room bathroom bedroom unlike kimpton calls suite, 5 day stay offer correct false advertising, send kimpton preferred guest website email asking failure provide suite advertised website reservation description furnished hard copy reservation printout website desk manager duty did not reply solution, send email trip guest survey did not follow email mail, guess tell concerned guest.the staff ranged indifferent not helpful, asked desk good breakfast spots neighborhood hood told no hotels, gee best breakfast spots seattle 1/2 block away convenient hotel does not know exist, arrived late night 11 pm inside run bellman busy chating cell phone help bags.prior arrival emailed hotel inform 20th anniversary half really picky wanted make sure good, got nice email saying like deliver bottle champagne chocolate covered strawberries room arrival celebrate, told needed foam pillows, arrival no champagne strawberries no foam pillows great room view alley high rise building good not better housekeeping staff cleaner room property, impressed left morning shopping room got short trips 2 hours, beds comfortable.not good ac-heat control 4 x 4 inch screen bring green shine directly eyes night, light sensitive tape controls.this not 4 start hotel clean business hotel super high rates, better chain hotels seattle,  '],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1.1: Data Preprocessing"
      ],
      "metadata": {
        "id": "xRaoDSR86k2m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing using Gensim's \"simple_preprocess\" library\n",
        "# this library lowercases, tokenizes, and performs \n",
        "# all other basic preprocesing to turn text into data\n",
        "def sent_to_words(sentences):\n",
        "    for sentence in sentences:\n",
        "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
        "\n",
        "# create one list per review, with all clean tokens\n",
        "data_words = list(sent_to_words(X))\n",
        "\n",
        "# confirm the output for 1 review\n",
        "print(data_words[2])\n",
        "\n",
        "# check the length of our list of lists. It should equal the number of rows with text\n",
        "# in the dataframe:\n",
        "print(len(data_words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WtKwjaaS6fDa",
        "outputId": "95eee819-e7c8-4b9f-dbb7-08b689fdac47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['nice', 'rooms', 'not', 'experience', 'hotel', 'monaco', 'seattle', 'good', 'hotel', 'level', 'positives', 'large', 'bathroom', 'mediterranean', 'suite', 'comfortable', 'bed', 'housekeeping', 'staffnegatives', 'ac', 'unit', 'malfunctioned', 'stay', 'desk', 'disorganized', 'missed', 'separate', 'wakeup', 'calls', 'concierge', 'busy', 'hard', 'touch', 'did', 'provide', 'guidance', 'special', 'requests', 'tv', 'hard', 'use', 'ipod', 'sound', 'dock', 'suite', 'non', 'functioning', 'decided', 'book', 'mediterranean', 'suite', 'night', 'weekend', 'stay', 'st', 'choice', 'rest', 'party', 'filled', 'comparison', 'spent', 'night', 'larger', 'square', 'footage', 'room', 'great', 'soaking', 'tub', 'whirlpool', 'jets', 'nice', 'shower', 'before', 'stay', 'hotel', 'arrange', 'car', 'service', 'price', 'tip', 'reasonable', 'driver', 'waiting', 'arrival', 'checkin', 'easy', 'downside', 'room', 'picked', 'person', 'jacuzi', 'tub', 'no', 'bath', 'accessories', 'salts', 'bubble', 'bath', 'did', 'stay', 'night', 'got', 'checked', 'voucher', 'bottle', 'champagne', 'nice', 'gesture', 'fish', 'waiting', 'room', 'impression', 'room', 'huge', 'open', 'space', 'felt', 'room', 'big', 'tv', 'far', 'away', 'bed', 'chore', 'change', 'channel', 'ipod', 'dock', 'broken', 'disappointing', 'in', 'morning', 'way', 'asked', 'desk', 'check', 'thermostat', 'said', 'degrees', 'warm', 'try', 'cover', 'face', 'night', 'bright', 'blue', 'light', 'kept', 'got', 'room', 'night', 'no', 'st', 'drop', 'desk', 'called', 'maintainence', 'came', 'look', 'thermostat', 'told', 'play', 'settings', 'happy', 'digital', 'box', 'wo', 'work', 'asked', 'wakeup', 'am', 'morning', 'did', 'happen', 'called', 'later', 'pm', 'nap', 'wakeup', 'forgot', 'am', 'wakeup', 'morning', 'yep', 'forgotten', 'the', 'bathroom', 'facilities', 'great', 'room', 'surprised', 'room', 'sold', 'whirlpool', 'bath', 'tub', 'bath', 'amenities', 'great', 'relax', 'water', 'jets', 'going']\n",
            "20491\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define functions for stopwordsand lemmatization\n",
        "\n",
        "# from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words = stopwords.words('english') \n",
        "\n",
        "\n",
        " # To check if clean_text function returned same results as remove_stopwords single line function\n",
        "def remove_stopwords(texts):\n",
        "   return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
        "\n",
        "\n",
        "# def clean_text(texts):\n",
        "#   result = []\n",
        "#   for doc in texts:\n",
        "#     inner_list = list()\n",
        "#     result.append(inner_list)\n",
        "#     for word in doc:\n",
        "#       if word not in stop_words:\n",
        "#         inner_list.append(word)\n",
        "#   return result\n",
        "\n",
        "\n",
        "# Function to lemmatize\n",
        "# We use a library called 'spacy' (https://spacy.io/) for this\n",
        "# To speeed up processing, we'll only lemmatize nouns, adjectives, verbs, and adverbs\n",
        "# since these are the most important words for polarity (remember we are processing text\n",
        "# deemed 'positive' in polarity)\n",
        "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
        "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
        "    texts_out = []\n",
        "    for sent in texts:\n",
        "        doc = nlp(\" \".join(sent)) \n",
        "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
        "    return texts_out\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8It5ZzF87e1",
        "outputId": "a9836386-a4df-4edf-be09-40880d473ffa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# process text\n",
        "# remove stopwords and create a variable with the clean corpus\n",
        "data_words_nostopwords = remove_stopwords(data_words)\n",
        "\n",
        "\n",
        "# # To check if new clean_text matched old remove_stopwords functions\n",
        "# data_words_clean = clean_text(data_words)\n",
        "# print(data_words_clean == compdata)\n",
        "# print(data_words_clean[:2])\n",
        "# print(compdata[:2])\n",
        "\n",
        "# initialize spacy's 'en_core_web_sm' package for Website text (https://spacy.io/models/en)\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "nlp.max_length = 2600000\n",
        "\n",
        "# extract our groups of words and lemmatize them\n",
        "data_lemmatized = lemmatization(data_words_nostopwords, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
        "\n",
        "print(data_lemmatized[:2])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L7yhR3ka87aX",
        "outputId": "9f4e7828-c382-4e5e-a024-17a441aaac0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['nice', 'hotel', 'expensive', 'parking', 'get', 'good', 'deal', 'stay', 'hotel', 'anniversary', 'arrive', 'late', 'evening', 'take', 'advice', 'previous', 'review', 'valet', 'parking', 'check', 'quick', 'easy', 'little', 'disappointed', 'non', 'existent', 'view', 'room', 'room', 'clean', 'nice', 'size', 'bed', 'comfortable', 'wake', 'stiff', 'neck', 'high', 'pillow', 'soundproof', 'hear', 'music', 'room', 'night', 'morning', 'loud', 'bang', 'door', 'open', 'closing', 'hear', 'people', 'talk', 'hallway', 'maybe', 'noisy', 'neighbor', 'aveda', 'bath', 'product', 'nice', 'stay', 'nice', 'touch', 'take', 'advantage', 'stay', 'long', 'location', 'great', 'walk', 'distance', 'shop', 'overall', 'nice', 'experience', 'pay', 'parking', 'night'], ['special', 'charge', 'diamond', 'member', 'decide', 'chain', 'shoot', 'anniversary', 'seattle', 'start', 'book', 'suite', 'pay', 'extra', 'website', 'description', 'suite', 'bedroom', 'bathroom', 'standard', 'hotel', 'room', 'take', 'print', 'reservation', 'desk', 'show', 'say', 'thing', 'tv', 'couch', 'ect', 'desk', 'clerk', 'tell', 'mixed', 'suite', 'free', 'breakfast', 'kid', 'embassy', 'suit', 'sit', 'room', 'bathroom', 'bedroom', 'call', 'day', 'stay', 'offer', 'correct', 'false', 'advertising', 'send', 'prefer', 'guest', 'website', 'email', 'ask', 'failure', 'provide', 'suite', 'advertised', 'website', 'reservation', 'description', 'furnish', 'hard', 'copy', 'reservation', 'manager', 'reply', 'solution', 'send', 'email', 'trip', 'guest', 'survey', 'follow', 'email', 'mail', 'guess', 'tell', 'concerned', 'guest', 'staff', 'range', 'indifferent', 'helpful', 'ask', 'desk', 'good', 'breakfast', 'spot', 'neighborhood', 'hood', 'tell', 'hotel', 'gee', 'good', 'breakfast', 'spot', 'seattle', 'block', 'away', 'convenient', 'hotel', 'know', 'exist', 'arrive', 'late', 'night', 'inside', 'run', 'bellman', 'busy', 'chat', 'cell', 'phone', 'help', 'bag', 'prior', 'arrival', 'email', 'hotel', 'inform', 'anniversary', 'half', 'really', 'picky', 'want', 'make', 'sure', 'good', 'get', 'nice', 'email', 'say', 'deliver', 'bottle', 'champagne', 'chocolate', 'cover', 'strawberry', 'room', 'arrival', 'tell', 'need', 'foam', 'pillow', 'arrival', 'foam', 'pillow', 'great', 'room', 'view', 'alley', 'high', 'rise', 'build', 'good', 'well', 'housekeeping', 'staff', 'clean', 'room', 'property', 'impressed', 'left', 'morning', 'shopping', 'room', 'get', 'short', 'trip', 'hour', 'bed', 'comfortable', 'good', 'heat', 'control', 'inch', 'screen', 'bring', 'green', 'shine', 'directly', 'eye', 'night', 'sensitive', 'tape', 'control', 'start', 'hotel', 'clean', 'business', 'hotel', 'super', 'high', 'rate', 'well', 'chain', 'hotel', 'seattle']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CHeck to make sure data shape remained the same\n",
        "print(og_len)\n",
        "new_len = len(data_lemmatized)\n",
        "print(new_len)\n",
        "print(og_len == new_len)\n"
      ],
      "metadata": {
        "id": "rkWbqvTTC8am",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f70e383c-a496-4b0b-f729-77b3a32cef71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20491\n",
            "20491\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_clean = []\n",
        "for sent in data_lemmatized:\n",
        "  combsent = \" \".join(sent)\n",
        "  X_clean.append(combsent)\n",
        "\n",
        "print(len(X_clean))\n",
        "print(X_clean[:3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHTDtTTnpDUr",
        "outputId": "ce0c4c01-3f13-4059-b77e-3b25e6d8e791"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20491\n",
            "['nice hotel expensive parking get good deal stay hotel anniversary arrive late evening take advice previous review valet parking check quick easy little disappointed non existent view room room clean nice size bed comfortable wake stiff neck high pillow soundproof hear music room night morning loud bang door open closing hear people talk hallway maybe noisy neighbor aveda bath product nice stay nice touch take advantage stay long location great walk distance shop overall nice experience pay parking night', 'special charge diamond member decide chain shoot anniversary seattle start book suite pay extra website description suite bedroom bathroom standard hotel room take print reservation desk show say thing tv couch ect desk clerk tell mixed suite free breakfast kid embassy suit sit room bathroom bedroom call day stay offer correct false advertising send prefer guest website email ask failure provide suite advertised website reservation description furnish hard copy reservation manager reply solution send email trip guest survey follow email mail guess tell concerned guest staff range indifferent helpful ask desk good breakfast spot neighborhood hood tell hotel gee good breakfast spot seattle block away convenient hotel know exist arrive late night inside run bellman busy chat cell phone help bag prior arrival email hotel inform anniversary half really picky want make sure good get nice email say deliver bottle champagne chocolate cover strawberry room arrival tell need foam pillow arrival foam pillow great room view alley high rise build good well housekeeping staff clean room property impressed left morning shopping room get short trip hour bed comfortable good heat control inch screen bring green shine directly eye night sensitive tape control start hotel clean business hotel super high rate well chain hotel seattle', 'nice room experience good hotel level positive large bathroom comfortable bed housekeeping staffnegative malfunction desk disorganize miss separate call concierge busy hard touch provide guidance special request tv hard use ipod sound suite non functioning decide book night weekend stay choice rest fill comparison spend night large square footage room great soak tub whirlpool jet nice stay hotel arrange car service price tip reasonable driver wait arrival downside room pick person bath accessory salt bubble bath stay night check voucher bottle champagne nice gesture fish waiting room impression room huge open space feel room big tv far away bed chore break disappointing morning way ask desk say degree warm cover face night bright blue light keep get room night desk call maintainence come look tell play setting happy work ask happen call later forget bathroom facility great room surprised room sell whirlpool bath tub bath amenity great relax water jet go']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DNmSmgzv76D"
      },
      "source": [
        "# Step 2: Split train/test data for hold-out test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NDPwcbD1v76E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b949042-7f5e-4ad1-818d-0b20a067a39b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12294\n",
            "8197\n",
            "12294\n",
            "8197\n"
          ]
        }
      ],
      "source": [
        "# check the sklearn documentation for train_test_split\n",
        "# http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
        "# \"test_size\" : float, int, None, optional\n",
        "# If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the test split. \n",
        "# If int, represents the absolute number of test samples. \n",
        "# If None, the value is set to the complement of the train size. \n",
        "# By default, the value is set to 0.25. The default will change in version 0.21. It will remain 0.25 only if train_size is unspecified, otherwise it will complement the specified train_size.    \n",
        "\n",
        "test_size = 0.4\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_clean, y, test_size=test_size, random_state=0)\n",
        "\n",
        "print(len(X_train))\n",
        "print(len(X_test))\n",
        "print(len(y_train))\n",
        "print(len(y_test))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9CtbMrQ1v76F"
      },
      "source": [
        "# Step 2.1 Data Checking"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "unique, counts = np.unique(y, return_counts=True)\n",
        "print(np.asarray((unique, counts)))"
      ],
      "metadata": {
        "id": "yHm7BlBhmHmz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5ad6abb-6902-4632-c02b-14069001c2bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[   1    2    3    4    5]\n",
            " [1421 1793 2184 6039 9054]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "plt.hist(y, bins = [0,1,2,3,4,5])"
      ],
      "metadata": {
        "id": "1P5KMHmamEDs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78093d49-5378-4943-f5d6-dd5cd02351c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([    0.,  1421.,  1793.,  2184., 15093.]),\n",
              " array([0., 1., 2., 3., 4., 5.]),\n",
              " <BarContainer object of 5 artists>)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAD4CAYAAAAO9oqkAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAAsTAAALEwEAmpwYAAASAklEQVR4nO3df6zddX3H8edrrfgDfxTkjrG22W1mw4Jkm3gDLCxmga0UNJY/1EA2qa6xf4ibThMFl4wMZJFsESVTlk46izNUghoaQbEBDDGRH7f8horc4Y/eBuzVFtQZdcX3/jifbsd6b9t7zr33tD3PR3Jyvt/39/P9nvc3hL7u99c5qSokScPttwbdgCRp8AwDSZJhIEkyDCRJGAaSJGDxoBvo1QknnFCjo6ODbkOSjijbtm37YVWN7F8/YsNgdHSU8fHxQbchSUeUJN+bru5pIkmSYSBJMgwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAkcQQ/gSxJo5feOugWFtx3P/rGedmuRwaSJMNAkmQYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiSOIQwSLIxya4kj02z7ANJKskJbT5Jrk0ykeSRJKd1jV2b5Kn2WttVf32SR9s61ybJXO2cJOnQHMqRwWeA1fsXkywHVgHf7yqfB6xsr/XAdW3s8cDlwBnA6cDlSY5r61wHvKtrvd/4LEnS/DpoGFTV3cDuaRZdA3wQqK7aGuCG6rgHWJLkJOBcYGtV7a6qPcBWYHVb9sqquqeqCrgBuKCvPZIkzVpP1wySrAF2VtXD+y1aCuzomp9stQPVJ6epz/S565OMJxmfmprqpXVJ0jRmHQZJXgZ8GPiHuW/nwKpqQ1WNVdXYyMjIQn+8JB21ejky+H1gBfBwku8Cy4AHkvwOsBNY3jV2WasdqL5smrokaQHNOgyq6tGq+u2qGq2qUTqndk6rqmeBLcDF7a6iM4Hnq+oZ4HZgVZLj2oXjVcDtbdmPk5zZ7iK6GLhljvZNknSIDuXW0huBbwInJ5lMsu4Aw28DngYmgH8H3g1QVbuBK4H72+uKVqON+XRb57+Ar/S2K5KkXh30N5Cr6qKDLB/tmi7gkhnGbQQ2TlMfB049WB+SpPnjE8iSJMNAkmQYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAkcWi/gbwxya4kj3XV/jnJt5I8kuRLSZZ0LbssyUSSJ5Oc21Vf3WoTSS7tqq9Icm+rfz7JMXO4f5KkQ3AoRwafAVbvV9sKnFpVfwh8G7gMIMkpwIXAa9s6n0qyKMki4JPAecApwEVtLMDVwDVV9RpgD7Curz2SJM3aQcOgqu4Gdu9X+1pV7W2z9wDL2vQaYHNV/aKqvgNMAKe310RVPV1VvwQ2A2uSBDgbuLmtvwm4oL9dkiTN1lxcM/hr4Ctteimwo2vZZKvNVH818FxXsOyrTyvJ+iTjScanpqbmoHVJEvQZBkn+HtgLfG5u2jmwqtpQVWNVNTYyMrIQHylJQ2FxrysmeQfwJuCcqqpW3gks7xq2rNWYof4jYEmSxe3ooHu8JGmB9HRkkGQ18EHgzVX1s65FW4ALk7w4yQpgJXAfcD+wst05dAydi8xbWojcBbylrb8WuKW3XZEk9epQbi29EfgmcHKSySTrgH8FXgFsTfJQkn8DqKrHgZuAJ4CvApdU1Qvtr/73ALcD24Gb2liADwHvTzJB5xrC9XO6h5KkgzroaaKqumia8oz/YFfVVcBV09RvA26bpv40nbuNJEkD4hPIkiTDQJJkGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJHFov4G8McmuJI911Y5PsjXJU+39uFZPkmuTTCR5JMlpXeusbeOfSrK2q/76JI+2da5NkrneSUnSgR3KkcFngNX71S4F7qiqlcAdbR7gPGBle60HroNOeACXA2fQ+b3jy/cFSBvzrq719v8sSdI8O2gYVNXdwO79ymuATW16E3BBV/2G6rgHWJLkJOBcYGtV7a6qPcBWYHVb9sqquqeqCriha1uSpAXS6zWDE6vqmTb9LHBim14K7OgaN9lqB6pPTlOXJC2gvi8gt7/oaw56Oagk65OMJxmfmppaiI+UpKHQaxj8oJ3iob3vavWdwPKuccta7UD1ZdPUp1VVG6pqrKrGRkZGemxdkrS/XsNgC7DvjqC1wC1d9YvbXUVnAs+300m3A6uSHNcuHK8Cbm/LfpzkzHYX0cVd25IkLZDFBxuQ5Ebgz4ATkkzSuSvoo8BNSdYB3wPe1obfBpwPTAA/A94JUFW7k1wJ3N/GXVFV+y5Kv5vOHUsvBb7SXpKkBXTQMKiqi2ZYdM40Ywu4ZIbtbAQ2TlMfB049WB+SpPnjE8iSJMNAkmQYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkugzDJL8XZLHkzyW5MYkL0myIsm9SSaSfD7JMW3si9v8RFs+2rWdy1r9ySTn9rlPkqRZ6jkMkiwF/hYYq6pTgUXAhcDVwDVV9RpgD7CurbIO2NPq17RxJDmlrfdaYDXwqSSLeu1LkjR7/Z4mWgy8NMli4GXAM8DZwM1t+Sbggja9ps3Tlp+TJK2+uap+UVXfASaA0/vsS5I0Cz2HQVXtBP4F+D6dEHge2AY8V1V727BJYGmbXgrsaOvubeNf3V2fZp1fk2R9kvEk41NTU722LknaTz+niY6j81f9CuB3gWPpnOaZN1W1oarGqmpsZGRkPj9KkoZKP6eJ/hz4TlVNVdX/AF8EzgKWtNNGAMuAnW16J7AcoC1/FfCj7vo060iSFkA/YfB94MwkL2vn/s8BngDuAt7SxqwFbmnTW9o8bfmdVVWtfmG722gFsBK4r4++JEmztPjgQ6ZXVfcmuRl4ANgLPAhsAG4FNif5SKtd31a5HvhskglgN507iKiqx5PcRCdI9gKXVNULvfYlSZq9nsMAoKouBy7fr/w009wNVFU/B946w3auAq7qpxdJUu98AlmSZBhIkgwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkjAMJEkYBpIk+gyDJEuS3JzkW0m2J/mTJMcn2ZrkqfZ+XBubJNcmmUjySJLTurazto1/KsnafndKkjQ7/R4ZfAL4alX9AfBHwHbgUuCOqloJ3NHmAc4DVrbXeuA6gCTH0/kd5TPo/Hby5fsCRJK0MHoOgySvAt4AXA9QVb+squeANcCmNmwTcEGbXgPcUB33AEuSnAScC2ytqt1VtQfYCqzutS9J0uz1c2SwApgC/iPJg0k+neRY4MSqeqaNeRY4sU0vBXZ0rT/ZajPVf0OS9UnGk4xPTU310bokqVs/YbAYOA24rqpeB/w3/39KCICqKqD6+IxfU1UbqmqsqsZGRkbmarOSNPT6CYNJYLKq7m3zN9MJhx+00z+0911t+U5gedf6y1ptprokaYH0HAZV9SywI8nJrXQO8ASwBdh3R9Ba4JY2vQW4uN1VdCbwfDuddDuwKslx7cLxqlaTJC2QxX2u/zfA55IcAzwNvJNOwNyUZB3wPeBtbextwPnABPCzNpaq2p3kSuD+Nu6KqtrdZ1+SpFnoKwyq6iFgbJpF50wztoBLZtjORmBjP71IknrnE8iSJMNAkmQYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkpiDMEiyKMmDSb7c5lckuTfJRJLPJzmm1V/c5ifa8tGubVzW6k8mObffniRJszMXRwbvBbZ3zV8NXFNVrwH2AOtafR2wp9WvaeNIcgpwIfBaYDXwqSSL5qAvSdIh6isMkiwD3gh8us0HOBu4uQ3ZBFzQpte0edryc9r4NcDmqvpFVX0HmABO76cvSdLs9Htk8HHgg8Cv2vyrgeeqam+bnwSWtumlwA6Atvz5Nv7/6tOs82uSrE8ynmR8amqqz9YlSfv0HAZJ3gTsqqptc9jPAVXVhqoaq6qxkZGRhfpYSTrqLe5j3bOANyc5H3gJ8ErgE8CSJIvbX//LgJ1t/E5gOTCZZDHwKuBHXfV9uteRJC2Ano8MquqyqlpWVaN0LgDfWVV/CdwFvKUNWwvc0qa3tHna8jurqlr9wna30QpgJXBfr31JkmavnyODmXwI2JzkI8CDwPWtfj3w2SQTwG46AUJVPZ7kJuAJYC9wSVW9MA99SZJmMCdhUFVfB77epp9mmruBqurnwFtnWP8q4Kq56EWSNHs+gSxJMgwkSYaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiQMA0kSfYRBkuVJ7kryRJLHk7y31Y9PsjXJU+39uFZPkmuTTCR5JMlpXdta28Y/lWRt/7slSZqNfn4DeS/wgap6IMkrgG1JtgLvAO6oqo8muRS4FPgQcB6wsr3OAK4DzkhyPHA5MAZU286WqtrTR2/SUBq99NZBt6AjVM9HBlX1TFU90KZ/AmwHlgJrgE1t2Cbggja9BrihOu4BliQ5CTgX2FpVu1sAbAVW99qXJGn25uSaQZJR4HXAvcCJVfVMW/QscGKbXgrs6FptstVmqk/3OeuTjCcZn5qamovWJUnMQRgkeTnwBeB9VfXj7mVVVXRO/cyJqtpQVWNVNTYyMjJXm5WkoddXGCR5EZ0g+FxVfbGVf9BO/9Ded7X6TmB51+rLWm2muiRpgfR8ATlJgOuB7VX1sa5FW4C1wEfb+y1d9fck2UznAvLzVfVMktuBf9p31xGwCris176kfbyYKh26fu4mOgt4O/Bokoda7cN0QuCmJOuA7wFva8tuA84HJoCfAe8EqKrdSa4E7m/jrqiq3X30JUmapZ7DoKq+AWSGxedMM76AS2bY1kZgY6+9SJL64xPIkiTDQJLU3zUDHUG8mCrpQDwykCQZBpIkw0CShGEgScIwkCRhGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkDqMwSLI6yZNJJpJcOuh+JGmYHBZhkGQR8EngPOAU4KIkpwy2K0kaHodFGACnAxNV9XRV/RLYDKwZcE+SNDQOl99AXgrs6JqfBM7Yf1CS9cD6NvvTJE/2+HknAD/scd0jlfs8HIZtn4dtf8nVfe/z701XPFzC4JBU1QZgQ7/bSTJeVWNz0NIRw30eDsO2z8O2vzB/+3y4nCbaCSzvml/WapKkBXC4hMH9wMokK5IcA1wIbBlwT5I0NA6L00RVtTfJe4DbgUXAxqp6fB4/su9TTUcg93k4DNs+D9v+wjztc6pqPrYrSTqCHC6niSRJA2QYSJKGKwyG8SsvkmxMsivJY4PuZSEkWZ7kriRPJHk8yXsH3dN8S/KSJPclebjt8z8OuqeFkmRRkgeTfHnQvSyEJN9N8miSh5KMz+m2h+WaQfvKi28Df0Hnobb7gYuq6omBNjbPkrwB+ClwQ1WdOuh+5luSk4CTquqBJK8AtgEXHM3/nZMEOLaqfprkRcA3gPdW1T0Dbm3eJXk/MAa8sqreNOh+5luS7wJjVTXnD9oN05HBUH7lRVXdDewedB8LpaqeqaoH2vRPgO10nnA/alXHT9vsi9rrqP8rL8ky4I3Apwfdy9FgmMJguq+8OKr/kRh2SUaB1wH3DriVeddOlzwE7AK2VtVRv8/Ax4EPAr8acB8LqYCvJdnWvp5nzgxTGGiIJHk58AXgfVX140H3M9+q6oWq+mM6T++fnuSoPiWY5E3ArqraNuheFtifVtVpdL7h+ZJ2GnhODFMY+JUXQ6KdN/8C8Lmq+uKg+1lIVfUccBewesCtzLezgDe3c+ibgbOT/OdgW5p/VbWzve8CvkTn9PecGKYw8CsvhkC7mHo9sL2qPjbofhZCkpEkS9r0S+ncJPGtgTY1z6rqsqpaVlWjdP5fvrOq/mrAbc2rJMe2myJIciywCpizuwSHJgyqai+w7ysvtgM3zfNXXhwWktwIfBM4OclkknWD7mmenQW8nc5fig+11/mDbmqenQTcleQROn/0bK2qobjVcsicCHwjycPAfcCtVfXVudr40NxaKkma2dAcGUiSZmYYSJIMA0mSYSBJwjCQJGEYSJIwDCRJwP8C5sJpgDshuikAAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fLUuctz4v76F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3468b149-834f-4c31-9744-c02a0dcf28f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[   1    2    3    4    5]\n",
            " [ 876 1088 1253 3598 5479]]\n"
          ]
        }
      ],
      "source": [
        "# Check how many training examples in each category\n",
        "# Distribution of sentiment labels on training set\n",
        "unique, counts = np.unique(y_train, return_counts=True)\n",
        "print(np.asarray((unique, counts)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LKcLzM6jpSSO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist(y_train, bins=5)"
      ],
      "metadata": {
        "id": "D_AP8MdEmRP6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "473827a7-a87f-495e-c279-0e2f0324fc92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([ 876., 1088., 1253., 3598., 5479.]),\n",
              " array([1. , 1.8, 2.6, 3.4, 4.2, 5. ]),\n",
              " <BarContainer object of 5 artists>)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5KrNZeo1v76G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5acdeac0-3743-45a0-f0ca-8406cb131778"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[   1    2    3    4    5]\n",
            " [ 545  705  931 2441 3575]]\n"
          ]
        }
      ],
      "source": [
        "# Distribution of sentiment labels on test set\n",
        "unique, counts = np.unique(y_test, return_counts=True)\n",
        "print(np.asarray((unique, counts)))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist(y_test, bins = [0,1,2,3,4,5])"
      ],
      "metadata": {
        "id": "tHqscJHrmVDF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "outputId": "74f61eb6-db2e-495a-a853-4ce46b9528d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([   0.,  545.,  705.,  931., 6016.]),\n",
              " array([0., 1., 2., 3., 4., 5.]),\n",
              " <BarContainer object of 5 artists>)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yu8M9Gstv76G"
      },
      "source": [
        "# Step 3: Vectorization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xTGd3qm7v76H"
      },
      "outputs": [],
      "source": [
        "# sklearn contains two vectorizers\n",
        "\n",
        "# CountVectorizer can give you Boolean or TF vectors\n",
        "# http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
        "\n",
        "# TfidfVectorizer can give you TF or TFIDF vectors\n",
        "# http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
        "\n",
        "# Read the sklearn documentation to understand all vectorization options\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# several commonly used vectorizer setting\n",
        "\n",
        "#  unigram boolean vectorizer, set minimum document frequency to 5\n",
        "unigram_bool_vectorizer = CountVectorizer(encoding='latin-1', binary=True, min_df=5, stop_words='english')\n",
        "\n",
        "#  unigram term frequency vectorizer, set minimum document frequency to 5\n",
        "unigram_count_vectorizer = CountVectorizer(encoding='latin-1', binary=False, min_df=5, stop_words='english')\n",
        "\n",
        "#  unigram and trigram term frequency vectorizer, set minimum document frequency to 5\n",
        "trigram_count_vectorizer = CountVectorizer(encoding='latin-1', ngram_range=(1,3), min_df=5, stop_words='english')\n",
        "\n",
        "#  unigram tfidf vectorizer, set minimum document frequency to 5\n",
        "unigram_tfidf_vectorizer = TfidfVectorizer(encoding='latin-1', use_idf=True, min_df=5, stop_words='english')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70gLo8_2v76I"
      },
      "source": [
        "## Step 3.1: Vectorize the training data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Unigram count vectorizer'''\n",
        "# fit vocabulary in training documents and transform the training documents into vectors\n",
        "X_train_vec = unigram_count_vectorizer.fit_transform(X_train)\n",
        "\n",
        "# check the content of a document vector\n",
        "print(X_train_vec.shape)\n",
        "print(X_train_vec[0].toarray())\n",
        "\n",
        "# check the size of the constructed vocabulary\n",
        "print(len(unigram_count_vectorizer.vocabulary_))\n",
        "\n",
        "# print out the first 10 items in the vocabulary\n",
        "print(list(unigram_count_vectorizer.vocabulary_.items())[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TcW0DFeCmr4T",
        "outputId": "698f3031-4dbc-4275-8c3a-1939e772d4f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(12294, 6840)\n",
            "[[0 0 0 ... 0 0 0]]\n",
            "6840\n",
            "[('earth', 1918), ('really', 4819), ('think', 6122), ('word', 6777), ('break', 737), ('dirty', 1700), ('smelly', 5549), ('room', 5122), ('toilet', 6185), ('area', 324)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''trigram count vectorizer'''\n",
        "# fit vocabulary in training documents and transform the training documents into vectors\n",
        "X_train_vec_trigram = trigram_count_vectorizer.fit_transform(X_train)\n",
        "\n",
        "# check the content of a document vector\n",
        "print(X_train_vec_trigram.shape)\n",
        "print(X_train_vec_trigram[0].toarray())\n",
        "\n",
        "# check the size of the constructed vocabulary\n",
        "print(len(trigram_count_vectorizer.vocabulary_))\n",
        "\n",
        "# print out the first 10 items in the vocabulary\n",
        "print(list(trigram_count_vectorizer.vocabulary_.items())[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0vNH_BJ1mspK",
        "outputId": "69304d7a-1243-4427-9be6-2bc30f2d7c48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(12294, 41828)\n",
            "[[0 0 0 ... 0 0 0]]\n",
            "41828\n",
            "[('earth', 9936), ('really', 27911), ('think', 37094), ('word', 41460), ('break', 3981), ('dirty', 9167), ('smelly', 33562), ('room', 29994), ('toilet', 37730), ('area', 947)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ey0AniZBv76J",
        "outputId": "0f6b3df7-bff0-4e3b-8fa4-ff39eb7a9c62",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(12294, 6840)\n",
            "[[0. 0. 0. ... 0. 0. 0.]]\n",
            "6840\n",
            "[('earth', 1918), ('really', 4819), ('think', 6122), ('word', 6777), ('break', 737), ('dirty', 1700), ('smelly', 5549), ('room', 5122), ('toilet', 6185), ('area', 324)]\n"
          ]
        }
      ],
      "source": [
        "'''TFIDF vectorizer'''\n",
        "# fit vocabulary in training documents and transform the training documents into vectors\n",
        "X_train_vec_tfidf = unigram_tfidf_vectorizer.fit_transform(X_train)\n",
        "\n",
        "# check the content of a document vector\n",
        "print(X_train_vec_tfidf.shape)\n",
        "print(X_train_vec_tfidf[0].toarray())\n",
        "\n",
        "# check the size of the constructed vocabulary\n",
        "print(len(unigram_tfidf_vectorizer.vocabulary_))\n",
        "\n",
        "# print out the first 10 items in the vocabulary\n",
        "print(list(unigram_tfidf_vectorizer.vocabulary_.items())[:10])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3.2: Vectorize the bal training data"
      ],
      "metadata": {
        "id": "kKC51vdOzndG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Resample each vectorizer iteration to balance data set\n",
        "\n",
        "ros = RandomOverSampler(random_state=42)\n",
        "\n",
        "X_train_vec_bal, y_train_bal = ros.fit_resample(X_train_vec.toarray(), y_train)\n",
        "X_train_vec_trigram_bal, y_train_bal = ros.fit_resample(X_train_vec_trigram.toarray(), y_train)\n",
        "X_train_vec_tfidf_bal, y_train_bal = ros.fit_resample(X_train_vec_tfidf.toarray(), y_train)\n",
        "\n",
        "print(len(y_train))\n",
        "print(len(y_train_bal))\n",
        "\n",
        "print(X_train_vec.shape[0])\n",
        "print(X_train_vec_bal.shape[0])\n",
        "\n",
        "# summarize distribution\n",
        "counter = Counter(y_train_bal)\n",
        "for k,v in counter.items():\n",
        " per = v / len(y) * 100\n",
        " print('Class=%d, n=%d (%.3f%%)' % (k, v, per))\n",
        "# plot the distribution\n",
        "plt.bar(counter.keys(), counter.values())\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-ExIcy7qJZT4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2uY-EfoBv76K"
      },
      "source": [
        "## Step 3.2: Vectorize the test data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Unigram count vectorizer'''\n",
        "# use the vocabulary constructed from the training data to vectorize the test data. \n",
        "# Therefore, use \"transform\" only, not \"fit_transform\", \n",
        "# otherwise \"fit\" would generate a new vocabulary from the test data\n",
        "\n",
        "X_test_vec = unigram_count_vectorizer.transform(X_test)\n",
        "\n",
        "# print out #examples and #features in the test set\n",
        "print(X_test_vec.shape)"
      ],
      "metadata": {
        "id": "xaqGR2ZrnnEM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''trigram Vectorizer'''\n",
        "# use the vocabulary constructed from the training data to vectorize the test data. \n",
        "# Therefore, use \"transform\" only, not \"fit_transform\", \n",
        "# otherwise \"fit\" would generate a new vocabulary from the test data\n",
        "\n",
        "X_test_vec_trigram = trigram_count_vectorizer.transform(X_test)\n",
        "\n",
        "# print out #examples and #features in the test set\n",
        "print(X_test_vec_trigram.shape)"
      ],
      "metadata": {
        "id": "DLGkR4e3nm7L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WXL2tFqzv76K"
      },
      "outputs": [],
      "source": [
        "'''TFIDF Vectorizer'''\n",
        "# use the vocabulary constructed from the training data to vectorize the test data. \n",
        "# Therefore, use \"transform\" only, not \"fit_transform\", \n",
        "# otherwise \"fit\" would generate a new vocabulary from the test data\n",
        "\n",
        "X_test_vec_tfidf = unigram_tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# print out #examples and #features in the test set\n",
        "print(X_test_vec_tfidf.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6qucH8zv76L"
      },
      "source": [
        "# Step 4: Train a MNB classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M686-ET0v76L"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jDCOnJ8vv76L"
      },
      "outputs": [],
      "source": [
        "'''MNB Model using TFIDF vectorizer'''\n",
        "\n",
        "# initialize the MNB model\n",
        "nb_clf= MultinomialNB()\n",
        "nb_clf_trigram = MultinomialNB()\n",
        "nb_clf_tfidf = MultinomialNB()\n",
        "\n",
        "\n",
        "nb_clf_bal= MultinomialNB()\n",
        "nb_clf_trigram_bal = MultinomialNB()\n",
        "nb_clf_tfidf_bal = MultinomialNB()\n",
        "\n",
        "# use the training data to train the MNB model\n",
        "nb_clf.fit(X_train_vec, y_train)  \n",
        "nb_clf_trigram.fit(X_train_vec_trigram, y_train)  \n",
        "nb_clf_tfidf.fit(X_train_vec_tfidf, y_train)  \n",
        "\n",
        "\n",
        "nb_clf_bal.fit(X_train_vec_bal, y_train_bal)  \n",
        "nb_clf_trigram_bal.fit(X_train_vec_trigram_bal, y_train_bal)  \n",
        "nb_clf_tfidf_bal.fit(X_train_vec_tfidf_bal, y_train_bal)  \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Unigram Feature Ranks - Negative '''\n",
        "feature_ranks = sorted(zip(nb_clf.feature_log_prob_[0], unigram_count_vectorizer.get_feature_names_out()))\n",
        "very_negative_features = feature_ranks[:10]\n",
        "print(\"Very negative words\")\n",
        "for i in range(0, len(very_negative_features)):\n",
        "    print(very_negative_features[i])"
      ],
      "metadata": {
        "id": "DtFXTgL1qU9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Unigram Feature Ranks - Positive '''\n",
        "feature_ranks = sorted(zip(nb_clf.feature_log_prob_[4], unigram_count_vectorizer.get_feature_names_out()))\n",
        "very_positive_features = feature_ranks[-10:]\n",
        "print(\"Very positive words\")\n",
        "for i in range(0, len(very_positive_features)):\n",
        "    print(very_positive_features[i])\n",
        "print()"
      ],
      "metadata": {
        "id": "plcKVx_nqU7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''trigram Feature Ranks - Negative '''\n",
        "feature_ranks = sorted(zip(nb_clf_trigram.feature_log_prob_[0],trigram_count_vectorizer.get_feature_names_out()))\n",
        "very_negative_features = feature_ranks[:10]\n",
        "print(\"Very negative words\")\n",
        "for i in range(0, len(very_negative_features)):\n",
        "    print(very_negative_features[i])"
      ],
      "metadata": {
        "id": "xUTAlHMrqU4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''trigram Feature Ranks - Positive '''\n",
        "feature_ranks = sorted(zip(nb_clf_trigram.feature_log_prob_[4], trigram_count_vectorizer.get_feature_names_out()))\n",
        "very_positive_features = feature_ranks[-10:]\n",
        "print(\"Very positive words\")\n",
        "for i in range(0, len(very_positive_features)):\n",
        "    print(very_positive_features[i])\n",
        "print()"
      ],
      "metadata": {
        "id": "C6l04L-3qU1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "02eCUY5Cv76M"
      },
      "outputs": [],
      "source": [
        "'''TFIDF Feature Ranks - Negative '''\n",
        "feature_ranks = sorted(zip(nb_clf_tfidf.feature_log_prob_[0], unigram_tfidf_vectorizer.get_feature_names_out()))\n",
        "very_negative_features = feature_ranks[-10:]\n",
        "print(\"Very negative words\")\n",
        "for i in range(0, len(very_negative_features)):\n",
        "    print(very_negative_features[i])\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RQuDy9-bv76M"
      },
      "outputs": [],
      "source": [
        "'''TFIDF Feature Ranks - Positive '''\n",
        "feature_ranks = sorted(zip(nb_clf_tfidf.feature_log_prob_[4], unigram_tfidf_vectorizer.get_feature_names_out()))\n",
        "very_positive_features = feature_ranks[-10:]\n",
        "print(\"Very positive words\")\n",
        "for i in range(0, len(very_positive_features)):\n",
        "    print(very_positive_features[i])\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aDbh-d-wv76N"
      },
      "outputs": [],
      "source": [
        "# test the classifier on the test data set, print accuracy score\n",
        "\n",
        "print('Unigram Score: ',nb_clf.score(X_test_vec, y_test))\n",
        "print('Trigram Score: ',nb_clf_trigram.score(X_test_vec_trigram, y_test))\n",
        "print('TFIDF Score: ',nb_clf_tfidf.score(X_test_vec_tfidf, y_test))\n",
        "\n",
        "print('Unigram Bal Score: ',nb_clf_bal.score(X_test_vec.toarray(), y_test))\n",
        "print('Trigram Bal Score: ',nb_clf_trigram_bal.score(X_test_vec_trigram.toarray(), y_test))\n",
        "print('TFIDF Bal Score: ',nb_clf_tfidf_bal.score(X_test_vec_tfidf.toarray(), y_test))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create lists to append returned values for model comparison at the end\n",
        "model_name = []\n",
        "model_type = []\n",
        "vector_type = []\n",
        "sample_type = []\n",
        "data_type = []\n",
        "accuracy = []\n",
        "precision_wavg = []\n",
        "recall_wavg = []\n",
        "f1_wavg = []"
      ],
      "metadata": {
        "id": "ksikE7zO6GP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Unigram '''\n",
        "# print confusion matrix (row: ground truth; col: prediction)\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "nb_y_pred = nb_clf.predict(X_test_vec)\n",
        "cm=confusion_matrix(y_test, nb_y_pred, labels=[0,1,2,3,4])\n",
        "print(cm)\n",
        "\n",
        "# print classification report\n",
        "### Unigram Count\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "target_names = ['1','2','3','4','5']\n",
        "print(classification_report(y_test, nb_y_pred, target_names=target_names))\n",
        "\n",
        "report = classification_report(y_test, nb_y_pred, target_names=target_names,output_dict=True)\n",
        "\n",
        "# Append model values to apply to a dataframe to compare model performance\n",
        "model_name.append('MNB-1C')\n",
        "model_type.append('MNB')\n",
        "vector_type.append('Unigram Count')\n",
        "sample_type.append('Hold-Out: '+ str(test_size))\n",
        "data_type.append('Original')\n",
        "accuracy.append(report['accuracy'])\n",
        "precision_wavg.append(report['weighted avg']['precision'])\n",
        "recall_wavg.append(report['weighted avg']['recall'])\n",
        "f1_wavg.append(report['weighted avg']['f1-score'])\n"
      ],
      "metadata": {
        "id": "tzujKkLstJmv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "nb_y_pred_bal = nb_clf_bal.predict(X_test_vec)\n",
        "cm=confusion_matrix(y_test, nb_y_pred_bal, labels=[0,1,2,3,4])\n",
        "print(cm)\n",
        "\n",
        "\n",
        "target_names = ['1','2','3','4','5']\n",
        "print(classification_report(y_test, nb_y_pred_bal, target_names=target_names))\n",
        "\n",
        "report = classification_report(y_test, nb_y_pred_bal, target_names=target_names,output_dict=True)\n",
        "\n",
        "# Append model values to apply to a dataframe to compare model performance\n",
        "model_name.append('MNB-1C')\n",
        "model_type.append('MNB')\n",
        "vector_type.append('Unigram Count')\n",
        "sample_type.append('Hold-Out: '+ str(test_size))\n",
        "data_type.append('Balanced')\n",
        "accuracy.append(report['accuracy'])\n",
        "precision_wavg.append(report['weighted avg']['precision'])\n",
        "recall_wavg.append(report['weighted avg']['recall'])\n",
        "f1_wavg.append(report['weighted avg']['f1-score'])"
      ],
      "metadata": {
        "id": "LdeS0zUqYiwE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''trigram '''\n",
        "# print confusion matrix (row: ground truth; col: prediction)\n",
        "\n",
        "nb_y_pred_trigram = nb_clf_trigram.predict(X_test_vec_trigram)\n",
        "cm=confusion_matrix(y_test, nb_y_pred_trigram, labels=[0,1,2,3,4])\n",
        "print(cm)\n",
        "\n",
        "# print classification report\n",
        "target_names = ['1','2','3','4','5']\n",
        "print(classification_report(y_test, nb_y_pred_trigram, target_names=target_names))\n",
        "\n",
        "report = classification_report(y_test, nb_y_pred_trigram, target_names=target_names,output_dict=True)\n",
        "\n",
        "# Append model values to apply to a dataframe to compare model performance\n",
        "model_name.append('MNB-3C')\n",
        "model_type.append('MNB')\n",
        "vector_type.append('trigram Count')\n",
        "sample_type.append('Hold-Out: '+ str(test_size))\n",
        "data_type.append('Original')\n",
        "accuracy.append(report['accuracy'])\n",
        "precision_wavg.append(report['weighted avg']['precision'])\n",
        "recall_wavg.append(report['weighted avg']['recall'])\n",
        "f1_wavg.append(report['weighted avg']['f1-score'])"
      ],
      "metadata": {
        "id": "q0P7hzAEtJj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print confusion matrix (row: ground truth; col: prediction)\n",
        "\n",
        "nb_y_pred_trigram_bal = nb_clf_trigram_bal.predict(X_test_vec_trigram)\n",
        "cm=confusion_matrix(y_test, nb_y_pred_trigram_bal, labels=[0,1,2,3,4])\n",
        "print(cm)\n",
        "\n",
        "# print classification report\n",
        "target_names = ['1','2','3','4','5']\n",
        "print(classification_report(y_test, nb_y_pred_trigram_bal, target_names=target_names))\n",
        "\n",
        "report = classification_report(y_test, nb_y_pred_trigram_bal, target_names=target_names,output_dict=True)\n",
        "\n",
        "# Append model values to apply to a dataframe to compare model performance\n",
        "model_name.append('MNB-3C')\n",
        "model_type.append('MNB')\n",
        "vector_type.append('trigram Count')\n",
        "sample_type.append('Hold-Out: '+ str(test_size))\n",
        "data_type.append('Balanced')\n",
        "accuracy.append(report['accuracy'])\n",
        "precision_wavg.append(report['weighted avg']['precision'])\n",
        "recall_wavg.append(report['weighted avg']['recall'])\n",
        "f1_wavg.append(report['weighted avg']['f1-score'])"
      ],
      "metadata": {
        "id": "tN9wFpxObB5k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "0OgIJ6Dqv76N"
      },
      "outputs": [],
      "source": [
        "'''TFIDF '''\n",
        "# print confusion matrix (row: ground truth; col: prediction)\n",
        "\n",
        "nb_y_pred_tfidf = nb_clf_tfidf.predict(X_test_vec_tfidf)\n",
        "cm=confusion_matrix(y_test, nb_y_pred_tfidf, labels=[0,1,2,3,4])\n",
        "print(cm)\n",
        "\n",
        "# print classification report\n",
        "target_names = ['1','2','3','4','5']\n",
        "print(classification_report(y_test, nb_y_pred_tfidf, target_names=target_names))\n",
        "\n",
        "report = classification_report(y_test, nb_y_pred_tfidf, target_names=target_names,output_dict=True)\n",
        "\n",
        "# Append model values to apply to a dataframe to compare model performance\n",
        "model_name.append('MNB-TFIDF')\n",
        "model_type.append('MNB')\n",
        "vector_type.append('TFIDF')\n",
        "sample_type.append('Hold-Out: '+ str(test_size))\n",
        "data_type.append('Original')\n",
        "accuracy.append(report['accuracy'])\n",
        "precision_wavg.append(report['weighted avg']['precision'])\n",
        "recall_wavg.append(report['weighted avg']['recall'])\n",
        "f1_wavg.append(report['weighted avg']['f1-score'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print confusion matrix (row: ground truth; col: prediction)\n",
        "\n",
        "nb_y_pred_tfidf_bal = nb_clf_tfidf_bal.predict(X_test_vec_tfidf)\n",
        "cm=confusion_matrix(y_test, nb_y_pred_tfidf_bal, labels=[0,1,2,3,4])\n",
        "print(cm)\n",
        "\n",
        "# print classification report\n",
        "target_names = ['1','2','3','4','5']\n",
        "print(classification_report(y_test, nb_y_pred_tfidf_bal, target_names=target_names))\n",
        "\n",
        "report = classification_report(y_test, nb_y_pred_tfidf_bal, target_names=target_names,output_dict=True)\n",
        "\n",
        "# Append model values to apply to a dataframe to compare model performance\n",
        "model_name.append('MNB-TFIDF')\n",
        "model_type.append('MNB')\n",
        "vector_type.append('TFIDF')\n",
        "sample_type.append('Hold-Out: '+ str(test_size))\n",
        "data_type.append('Balanced')\n",
        "accuracy.append(report['accuracy'])\n",
        "precision_wavg.append(report['weighted avg']['precision'])\n",
        "recall_wavg.append(report['weighted avg']['recall'])\n",
        "f1_wavg.append(report['weighted avg']['f1-score'])"
      ],
      "metadata": {
        "id": "ymjFzkefbw9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QW5IzK78v76N"
      },
      "source": [
        "# Train SVM Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3eeRK32vv76O"
      },
      "outputs": [],
      "source": [
        "'''SVN model using trigram count vectorizer'''\n",
        "# import the LinearSVC module\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "# initialize the LinearSVC model\n",
        "svm_clf = LinearSVC(C=1)\n",
        "svm_clf_trigram = LinearSVC(C=1)\n",
        "svm_clf_tfidf = LinearSVC(C=1)\n",
        "\n",
        "svm_clf_bal = LinearSVC(C=1)\n",
        "svm_clf_trigram_bal = LinearSVC(C=1)\n",
        "svm_clf_tfidf_bal = LinearSVC(C=1)\n",
        "\n",
        "\n",
        "# use the training data to train the model\n",
        "svm_clf.fit(X_train_vec,y_train)\n",
        "svm_clf_trigram.fit(X_train_vec_trigram,y_train)\n",
        "svm_clf_tfidf.fit(X_train_vec_tfidf,y_train)\n",
        "\n",
        "svm_clf_bal.fit(X_train_vec_bal,y_train_bal)\n",
        "svm_clf_trigram_bal.fit(X_train_vec_trigram_bal,y_train_bal)\n",
        "svm_clf_tfidf_bal.fit(X_train_vec_tfidf_bal,y_train_bal)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Unigram Feature Ranks - Negative '''\n",
        "feature_ranks = sorted(zip(svm_clf.coef_[0], unigram_count_vectorizer.get_feature_names_out()))\n",
        "very_negative_features = feature_ranks[:10]\n",
        "print(\"Very negative words\")\n",
        "for i in range(0, len(very_negative_features)):\n",
        "    print(very_negative_features[i])"
      ],
      "metadata": {
        "id": "jWMc5kZbvDS0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Unigram Feature Ranks - Positive '''\n",
        "feature_ranks = sorted(zip(svm_clf.coef_[4], unigram_count_vectorizer.get_feature_names_out()))\n",
        "very_positive_features = feature_ranks[-10:]\n",
        "print(\"Very positive words\")\n",
        "for i in range(0, len(very_positive_features)):\n",
        "    print(very_positive_features[i])\n",
        "print()"
      ],
      "metadata": {
        "id": "7K_t1ULkvDS0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''trigram Feature Ranks - Negative '''\n",
        "feature_ranks = sorted(zip(svm_clf_trigram.coef_[0],trigram_count_vectorizer.get_feature_names_out()))\n",
        "very_negative_features = feature_ranks[:10]\n",
        "print(\"Very negative words\")\n",
        "for i in range(0, len(very_negative_features)):\n",
        "    print(very_negative_features[i])"
      ],
      "metadata": {
        "id": "NeJ_hnxkvDS1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''trigram Feature Ranks - Positive '''\n",
        "feature_ranks = sorted(zip(svm_clf_trigram.coef_[4], trigram_count_vectorizer.get_feature_names_out()))\n",
        "very_positive_features = feature_ranks[-10:]\n",
        "print(\"Very positive words\")\n",
        "for i in range(0, len(very_positive_features)):\n",
        "    print(very_positive_features[i])\n",
        "print()"
      ],
      "metadata": {
        "id": "nN5jzMECvDS1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LIV-6X4uvDS1"
      },
      "outputs": [],
      "source": [
        "'''TFIDF Feature Ranks - Negative '''\n",
        "feature_ranks = sorted(zip(svm_clf_tfidf.coef_[0], unigram_tfidf_vectorizer.get_feature_names_out()))\n",
        "very_negative_features = feature_ranks[-10:]\n",
        "print(\"Very negative words\")\n",
        "for i in range(0, len(very_negative_features)):\n",
        "    print(very_negative_features[i])\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gijbTiq5vDS1"
      },
      "outputs": [],
      "source": [
        "'''TFIDF Feature Ranks - Positive '''\n",
        "feature_ranks = sorted(zip(svm_clf_tfidf.coef_[4], unigram_tfidf_vectorizer.get_feature_names_out()))\n",
        "very_positive_features = feature_ranks[-10:]\n",
        "print(\"Very positive words\")\n",
        "for i in range(0, len(very_positive_features)):\n",
        "    print(very_positive_features[i])\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jzwDOy-Gv76P"
      },
      "outputs": [],
      "source": [
        "# test the classifier on the test data set, print accuracy score\n",
        "\n",
        "print('Unigram Score: ',svm_clf.score(X_test_vec,y_test))\n",
        "print('trigram Score: ',svm_clf_trigram.score(X_test_vec_trigram,y_test))\n",
        "print('TFIDF Score: ',svm_clf_tfidf.score(X_test_vec_tfidf,y_test))\n",
        "\n",
        "print('Unigram Bal Score: ',svm_clf_bal.score(X_test_vec,y_test))\n",
        "print('trigram Bal Score: ',svm_clf_trigram_bal.score(X_test_vec_trigram,y_test))\n",
        "print('TFIDF Bal Score: ',svm_clf_tfidf_bal.score(X_test_vec_tfidf,y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHzqD_65v76P"
      },
      "outputs": [],
      "source": [
        "''' Unigram'''\n",
        "# print confusion matrix and classification report\n",
        "svm_y_pred = svm_clf.predict(X_test_vec)\n",
        "cm=confusion_matrix(y_test, svm_y_pred, labels=[0,1,2,3,4])\n",
        "print(cm)\n",
        "print()\n",
        "\n",
        "target_names = ['1','2','3','4','5']\n",
        "print(classification_report(y_test, svm_y_pred, target_names=target_names))\n",
        "\n",
        "report = classification_report(y_test, svm_y_pred, target_names=target_names,output_dict=True)\n",
        "\n",
        "# Append model values to apply to a dataframe to compare model performance\n",
        "model_name.append('SVM-1C')\n",
        "model_type.append('SVM')\n",
        "vector_type.append('Unigram Count')\n",
        "sample_type.append('Hold-Out: ' + str(test_size))\n",
        "data_type.append('Original')\n",
        "accuracy.append(report['accuracy'])\n",
        "precision_wavg.append(report['weighted avg']['precision'])\n",
        "recall_wavg.append(report['weighted avg']['recall'])\n",
        "f1_wavg.append(report['weighted avg']['f1-score'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "svm_y_pred_bal = svm_clf_bal.predict(X_test_vec)\n",
        "cm=confusion_matrix(y_test, svm_y_pred_bal, labels=[0,1,2,3,4])\n",
        "print(cm)\n",
        "print()\n",
        "\n",
        "target_names = ['1','2','3','4','5']\n",
        "print(classification_report(y_test, svm_y_pred_bal, target_names=target_names))\n",
        "\n",
        "report = classification_report(y_test, svm_y_pred_bal, target_names=target_names,output_dict=True)\n",
        "\n",
        "# Append model values to apply to a dataframe to compare model performance\n",
        "model_name.append('SVM-1C')\n",
        "model_type.append('SVM')\n",
        "vector_type.append('Unigram Count')\n",
        "sample_type.append('Hold-Out: ' + str(test_size))\n",
        "data_type.append('Balanced')\n",
        "accuracy.append(report['accuracy'])\n",
        "precision_wavg.append(report['weighted avg']['precision'])\n",
        "recall_wavg.append(report['weighted avg']['recall'])\n",
        "f1_wavg.append(report['weighted avg']['f1-score'])"
      ],
      "metadata": {
        "id": "X-nbFQWFdzUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' trigram'''\n",
        "# print confusion matrix and classification report\n",
        "\n",
        "svm_y_pred_trigram = svm_clf_trigram.predict(X_test_vec_trigram)\n",
        "cm=confusion_matrix(y_test, svm_y_pred_trigram, labels=[0,1,2,3,4])\n",
        "print(cm)\n",
        "print()\n",
        "\n",
        "target_names = ['1','2','3','4','5']\n",
        "print(classification_report(y_test, svm_y_pred_trigram, target_names=target_names))\n",
        "\n",
        "report = classification_report(y_test, svm_y_pred_trigram, target_names=target_names,output_dict=True)\n",
        "\n",
        "# Append model values to apply to a dataframe to compare model performance\n",
        "model_name.append('SVM-3C')\n",
        "model_type.append('SVM')\n",
        "vector_type.append('trigram Count')\n",
        "sample_type.append('Hold-Out: ' + str(test_size))\n",
        "data_type.append('Original')\n",
        "accuracy.append(report['accuracy'])\n",
        "precision_wavg.append(report['weighted avg']['precision'])\n",
        "recall_wavg.append(report['weighted avg']['recall'])\n",
        "f1_wavg.append(report['weighted avg']['f1-score'])"
      ],
      "metadata": {
        "id": "XCmwN_PRxD14"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print confusion matrix and classification report\n",
        "\n",
        "svm_y_pred_trigram_bal = svm_clf_trigram_bal.predict(X_test_vec_trigram)\n",
        "cm=confusion_matrix(y_test, svm_y_pred_trigram_bal, labels=[0,1,2,3,4])\n",
        "print(cm)\n",
        "print()\n",
        "\n",
        "target_names = ['1','2','3','4','5']\n",
        "print(classification_report(y_test, svm_y_pred_trigram_bal, target_names=target_names))\n",
        "\n",
        "report = classification_report(y_test, svm_y_pred_trigram_bal, target_names=target_names,output_dict=True)\n",
        "\n",
        "# Append model values to apply to a dataframe to compare model performance\n",
        "model_name.append('SVM-3C')\n",
        "model_type.append('SVM')\n",
        "vector_type.append('trigram Count')\n",
        "sample_type.append('Hold-Out: ' + str(test_size))\n",
        "data_type.append('Balanaced')\n",
        "accuracy.append(report['accuracy'])\n",
        "precision_wavg.append(report['weighted avg']['precision'])\n",
        "recall_wavg.append(report['weighted avg']['recall'])\n",
        "f1_wavg.append(report['weighted avg']['f1-score'])"
      ],
      "metadata": {
        "id": "AbdHYUOZfnXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' TFIDF'''\n",
        "# print confusion matrix and classification report\n",
        "\n",
        "svm_y_pred_tfidf = svm_clf_tfidf.predict(X_test_vec_tfidf)\n",
        "cm=confusion_matrix(y_test, svm_y_pred_tfidf, labels=[0,1,2,3,4])\n",
        "print(cm)\n",
        "print()\n",
        "\n",
        "target_names = ['1','2','3','4','5']\n",
        "print(classification_report(y_test, svm_y_pred_tfidf, target_names=target_names))\n",
        "\n",
        "report = classification_report(y_test, svm_y_pred_tfidf, target_names=target_names,output_dict=True)\n",
        "\n",
        "# Append model values to apply to a dataframe to compare model performance\n",
        "model_name.append('SVM-TFIDF')\n",
        "model_type.append('SVM')\n",
        "vector_type.append('TFIDF')\n",
        "sample_type.append('Hold-Out: '+ str(test_size))\n",
        "data_type.append('Original')\n",
        "accuracy.append(report['accuracy'])\n",
        "precision_wavg.append(report['weighted avg']['precision'])\n",
        "recall_wavg.append(report['weighted avg']['recall'])\n",
        "f1_wavg.append(report['weighted avg']['f1-score'])"
      ],
      "metadata": {
        "id": "wad6Hr8GxDuB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print confusion matrix and classification report\n",
        "\n",
        "svm_y_pred_tfidf_bal = svm_clf_tfidf_bal.predict(X_test_vec_tfidf)\n",
        "cm=confusion_matrix(y_test, svm_y_pred_tfidf_bal, labels=[0,1,2,3,4])\n",
        "print(cm)\n",
        "print()\n",
        "\n",
        "target_names = ['1','2','3','4','5']\n",
        "print(classification_report(y_test, svm_y_pred_tfidf_bal, target_names=target_names))\n",
        "\n",
        "report = classification_report(y_test, svm_y_pred_tfidf_bal, target_names=target_names,output_dict=True)\n",
        "\n",
        "# Append model values to apply to a dataframe to compare model performance\n",
        "model_name.append('SVM-TFIDF')\n",
        "model_type.append('SVM')\n",
        "vector_type.append('TFIDF')\n",
        "sample_type.append('Hold-Out: '+ str(test_size))\n",
        "data_type.append('Balanaced')\n",
        "accuracy.append(report['accuracy'])\n",
        "precision_wavg.append(report['weighted avg']['precision'])\n",
        "recall_wavg.append(report['weighted avg']['recall'])\n",
        "f1_wavg.append(report['weighted avg']['f1-score'])"
      ],
      "metadata": {
        "id": "LGbBE4gtgGUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bernoulli NB\n",
        "\n"
      ],
      "metadata": {
        "id": "CJ2sca9T1dVT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fit vocabulary in training documents and transform the training documents into vectors\n",
        "X_train_bool = unigram_bool_vectorizer.fit_transform(X_train)\n",
        "\n",
        "# check the content of a document vector\n",
        "print(X_train_bool.shape)\n",
        "print(X_train_bool[0].toarray())\n",
        "\n",
        "# check the size of the constructed vocabulary\n",
        "print(len(unigram_bool_vectorizer.vocabulary_))\n",
        "\n",
        "# print out the first 10 items in the vocabulary\n",
        "print(list(unigram_bool_vectorizer.vocabulary_.items()))"
      ],
      "metadata": {
        "id": "wsdVgnZB1c5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use the vocabulary constructed from the training data to vectorize the test data. \n",
        "X_test_bool = unigram_bool_vectorizer.transform(X_test)\n",
        "\n",
        "# print out #examples and #features in the test set\n",
        "print(X_test_bool.shape)"
      ],
      "metadata": {
        "id": "TJEzHLot1c15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import BernoulliNB\n",
        "# X_train_vec_bool = unigram_bool_vectorizer.fit_transform(X_train)\n",
        "bernoulliNB_clf = BernoulliNB()\n",
        "\n",
        "# use the training data to train the MNB model\n",
        "bernoulliNB_clf.fit(X_train_bool,y_train) "
      ],
      "metadata": {
        "id": "em6F2qXT2J5G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print confusion matrix (row: ground truth; col: prediction)\n",
        "### bool\n",
        "\n",
        "y_pred_bool = bernoulliNB_clf.fit(X_train_bool, y_train).predict(X_test_bool)\n",
        "cm=confusion_matrix(y_test, y_pred_bool, labels=[1,2,3,4,5])\n",
        "print(cm)\n",
        "\n",
        "# print classification report \n",
        "### bool\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "target_names = ['1', '2', '3', '4', '5']\n",
        "print(classification_report(y_test, y_pred_bool, target_names=target_names))\n",
        "\n",
        "report = classification_report(y_test, y_pred_bool, target_names=target_names,output_dict=True)\n",
        "\n",
        "# Append model values to apply to a dataframe to compare model performance\n",
        "model_name.append('NB Bern')\n",
        "model_type.append('Bernoulli')\n",
        "vector_type.append('Boolean')\n",
        "sample_type.append('Hold-Out: '+ str(test_size))\n",
        "accuracy.append(report['accuracy'])\n",
        "precision_wavg.append(report['weighted avg']['precision'])\n",
        "recall_wavg.append(report['weighted avg']['recall'])\n",
        "f1_wavg.append(report['weighted avg']['f1-score'])"
      ],
      "metadata": {
        "id": "oE7Cl4zD2J0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4DPwK36v76Q"
      },
      "source": [
        "# Step 6: write the prediction output to file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FmEM4Xqov76Q"
      },
      "outputs": [],
      "source": [
        "# y_pred=nb_clf.predict(X_test_vec)\n",
        "# output = open('/Users/michaelharper/prediction_output.csv', 'w')\n",
        "# for x, value in enumerate(y_pred):\n",
        "#   output.write(str(value) + '\\n') \n",
        "# output.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOgfFU_wv76Q"
      },
      "source": [
        "# Cross Validation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Unigram MNB'''\n",
        "# cross validation NB\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import cross_val_score\n",
        "nb_clf_pipe = Pipeline([('vect', unigram_count_vectorizer),('nb', MultinomialNB())])\n",
        "scores = cross_val_score(nb_clf_pipe, X, y, cv=10)\n",
        "avg=sum(scores)/len(scores)\n",
        "print(avg)\n",
        "\n",
        "model_name.append('MNB-1C_CV')\n",
        "model_type.append('MNB')\n",
        "vector_type.append('Unigram Count')\n",
        "sample_type.append('10-CV')\n",
        "accuracy.append(avg)"
      ],
      "metadata": {
        "id": "ko8OrWydxsvZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''trigram MNB'''\n",
        "# cross validation NB\n",
        "\n",
        "nb_clf_trigram_pipe = Pipeline([('vect', trigram_count_vectorizer),('nb', MultinomialNB())])\n",
        "scores = cross_val_score(nb_clf_trigram_pipe, X, y, cv=10)\n",
        "avg=sum(scores)/len(scores)\n",
        "print(avg)\n",
        "\n",
        "model_name.append('MNB-3C_CV')\n",
        "model_type.append('MNB')\n",
        "vector_type.append('trigram Count')\n",
        "sample_type.append('10-CV')\n",
        "accuracy.append(avg)"
      ],
      "metadata": {
        "id": "Fz3-cwVBxsn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''TFIDF MNB'''\n",
        "# cross validation NB\n",
        "\n",
        "nb_clf_tfidf_pipe = Pipeline([('vect', unigram_tfidf_vectorizer),('nb', MultinomialNB())])\n",
        "scores = cross_val_score(nb_clf_tfidf_pipe, X, y, cv=10)\n",
        "avg=sum(scores)/len(scores)\n",
        "print(avg)\n",
        "\n",
        "model_name.append('MNB-TFIDF_CV')\n",
        "model_type.append('MNB')\n",
        "vector_type.append('TFIDF')\n",
        "sample_type.append('10-CV')\n",
        "accuracy.append(avg)"
      ],
      "metadata": {
        "id": "sSAwoqlrxscb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Unigram SVM'''\n",
        "# cross validation SVM\n",
        "\n",
        "svm_clf_pipe = Pipeline([('vect', unigram_count_vectorizer),('svm', LinearSVC(C=0.5))])\n",
        "scores = cross_val_score(svm_clf_pipe, X, y, cv=10)\n",
        "avg=sum(scores)/len(scores)\n",
        "print(avg)\n",
        "\n",
        "# Append model values to apply to a dataframe to compare model performance\n",
        "model_name.append('SVM-1C_CV')\n",
        "model_type.append('SVM')\n",
        "vector_type.append('Unigram Count')\n",
        "sample_type.append('10-CV')\n",
        "accuracy.append(avg)"
      ],
      "metadata": {
        "id": "iGYwTEvdxsQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6pHqyGKov76Q"
      },
      "outputs": [],
      "source": [
        "'''trigram SVM'''\n",
        "# cross validation SVM\n",
        "\n",
        "svm_clf_trigram_pipe = Pipeline([('vect', trigram_count_vectorizer),('svm', LinearSVC(C=0.5))])\n",
        "scores = cross_val_score(svm_clf_trigram_pipe, X, y, cv=10)\n",
        "avg=sum(scores)/len(scores)\n",
        "print(avg)\n",
        "\n",
        "# Append model values to apply to a dataframe to compare model performance\n",
        "model_name.append('SVM-3C_CV')\n",
        "model_type.append('SVM')\n",
        "vector_type.append('trigram Count')\n",
        "sample_type.append('10-CV')\n",
        "accuracy.append(avg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-3DKusf_v76R"
      },
      "outputs": [],
      "source": [
        "'''TFIDF SVM'''\n",
        "# cross validation SVM\n",
        "\n",
        "svm_clf_tfidf_pipe = Pipeline([('vect', unigram_tfidf_vectorizer),('svm', LinearSVC(C=0.5))])\n",
        "scores = cross_val_score(svm_clf_tfidf_pipe, X, y, cv=10)\n",
        "avg=sum(scores)/len(scores)\n",
        "print(avg)\n",
        "\n",
        "# Append model values to apply to a dataframe to compare model performance\n",
        "model_name.append('SVM-TFIDF_CV')\n",
        "model_type.append('SVM')\n",
        "vector_type.append('TFIDF')\n",
        "sample_type.append('10-CV')\n",
        "accuracy.append(avg)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "''' BernoulliNB '''\n",
        "\n",
        "# cross validation SVM\n",
        "\n",
        "nb_bern_pipe = Pipeline([('vect', unigram_bool_vectorizer),('nb', BernoulliNB())])\n",
        "scores = cross_val_score(nb_bern_pipe, X, y, cv=10)\n",
        "avg=sum(scores)/len(scores)\n",
        "print(avg)\n",
        "\n",
        "# Append model values to apply to a dataframe to compare model performance\n",
        "model_name.append('NB-Bern_CV')\n",
        "model_type.append('SVM')\n",
        "vector_type.append('Bernoulli')\n",
        "sample_type.append('10-CV')\n",
        "accuracy.append(avg)\n"
      ],
      "metadata": {
        "id": "pmog4xfw2oUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dataframe from dictionary of saved model scores\n",
        "dict = {'Model Name': model_name, 'Model Type': model_type, 'Vector Type':vector_type,'Sample Type': sample_type,'Accuracy': accuracy, 'Avg Precision':precision_wavg, 'Avg Recall':recall_wavg, 'Avg F1':f1_wavg}\n",
        "\n",
        "comp_df = pd.DataFrame.from_dict(dict, orient='index')\n",
        "comp_df = comp_df.transpose()\n",
        "\n",
        "# CSV output\n",
        "comp_df.to_csv(\"drive/Shareddrives/IST_736_Final_Project/Model_Scores.csv\")\n",
        "# Show\n",
        "comp_df"
      ],
      "metadata": {
        "id": "e2UEHc6e4WeC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Accuracy by Model\n",
        "import seaborn as sns\n",
        "ax = sns.barplot(data=comp_df,x='Model Name', y ='Accuracy')\n",
        "ax.set_xticklabels(ax.get_xticklabels(),rotation = 40, ha='right')\n",
        "plt.ylim(bottom= min(comp_df['Accuracy'])*.9)\n",
        "plt.title('Accuracy by Model')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "f5PbQN8V-HPu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Determine highest Accuracy Score\n",
        "max_accuracy = comp_df[comp_df['Accuracy'] == max(comp_df['Accuracy'])]\n",
        "print('Model w/ the highest accuracy score: ', max_accuracy[['Model Name','Accuracy']])\n",
        "# Determine highest F1 Score\n",
        "ho_df = comp_df[comp_df['Sample Type']== ('Hold-Out: ' + str(test_size))]\n",
        "max_f1 = ho_df[ho_df['Avg F1'] == max(ho_df['Avg F1'])]\n",
        "print('Model w/ the highest F1 score: ', max_f1[['Model Name','Avg F1']])\n"
      ],
      "metadata": {
        "id": "Xf3fAsec-Tq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "u7hp5Xse-XM0"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "  This program shell reads phrase data for the kaggle phrase sentiment classification problem.\n",
    "  The input to the program is the path to the kaggle directory \"corpus\" and a limit number.\n",
    "  The program reads all of the kaggle phrases, and then picks a random selection of the limit number.\n",
    "  It creates a \"phrasedocs\" variable with a list of phrases consisting of a pair\n",
    "    with the list of tokenized words from the phrase and the label number from 1 to 4\n",
    "  It prints a few example phrases.\n",
    "  In comments, it is shown how to get word lists from the two sentiment lexicons:\n",
    "      subjectivity and LIWC, if you want to use them in your features\n",
    "  Your task is to generate features sets and train and test a classifier.\n",
    "\n",
    "  This version uses cross-validation with the Naive Bayes classifier in NLTK.\n",
    "  It computes the evaluation measures of precision, recall and F1 measure for each fold.\n",
    "  It also averages across folds and across labels.\n",
    "'''\n",
    "\n",
    "'''PLEASE UNZIP BEFORE RUNNING.  IT WONT PICK UP PROPER PATH WITHOUT DOING THIS FIRST'''\n",
    "\n",
    "# open python and nltk packages needed for processing\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.collocations import *\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## define feature definition functions\n",
    "# this function define features (keywords) of a document for a BOW/unigram baseline\n",
    "# each feature is 'V_(keyword)' and is true or false depending\n",
    "# on whether that keyword is in the document\n",
    "def document_features(document, word_features):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['V_{}'.format(word)] = (word in document_words)\n",
    "    return features\n",
    "\n",
    "#list of negation words to create a negation feature:\n",
    "negationwords = ['no', 'not', 'never', 'none', 'nowhere', 'nothing', 'noone', 'rather', 'hardly', 'scarcely', 'rarely', 'seldom', 'neither', 'nor']\n",
    "\n",
    "\n",
    "# Creating the NOT features function\n",
    "def NOT_features(document, word_features, negationwords):\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['V_{}'.format(word)] = False\n",
    "        features['V_NOT{}'.format(word)] = False\n",
    "    # go through document words in order\n",
    "    for i in range(0, len(document)):\n",
    "        word = document[i]\n",
    "        if ((i + 1) < len(document)) and ((word in negationwords) or (word.endswith(\"n't\"))):\n",
    "            i += 1\n",
    "            features['V_NOT{}'.format(document[i])] = (document[i] in word_features)\n",
    "        else:\n",
    "            features['V_{}'.format(word)] = (word in word_features)\n",
    "    return features\n",
    "\n",
    "#Defining the POS features function\n",
    "def POS_features(document, word_features):\n",
    "    document_words = set(document)\n",
    "    tagged_words = nltk.pos_tag(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    numNoun = 0\n",
    "    numVerb = 0\n",
    "    numAdj = 0\n",
    "    numAdverb = 0\n",
    "    for (word, tag) in tagged_words:\n",
    "        if tag.startswith('N'): numNoun += 1\n",
    "        if tag.startswith('V'): numVerb += 1\n",
    "        if tag.startswith('J'): numAdj += 1\n",
    "        if tag.startswith('R'): numAdverb += 1\n",
    "    features['nouns'] = numNoun\n",
    "    features['verbs'] = numVerb\n",
    "    features['adjectives'] = numAdj\n",
    "    features['adverbs'] = numAdverb\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## cross-validation ##\n",
    "# this function takes the number of folds, the feature sets and the labels\n",
    "# it iterates over the folds, using different sections for training and testing in turn\n",
    "#   it prints the performance for each fold and the average performance at the end\n",
    "def cross_validation_PRF(num_folds, featuresets, labels):\n",
    "    subset_size = int(len(featuresets)/num_folds)\n",
    "    print('\\nEach fold size:', subset_size)\n",
    "    # for the number of labels - start the totals lists with zeroes\n",
    "    num_labels = len(labels)\n",
    "    total_precision_list = [0] * num_labels\n",
    "    total_recall_list = [0] * num_labels\n",
    "    total_F1_list = [0] * num_labels\n",
    "    \n",
    "    # iterate over the folds\n",
    "    for i in range(num_folds):\n",
    "        test_this_round = featuresets[(i*subset_size):][:subset_size]\n",
    "        train_this_round = featuresets[:(i*subset_size)] + featuresets[((i+1)*subset_size):]\n",
    "        # train using train_this_round\n",
    "        classifier = nltk.NaiveBayesClassifier.train(train_this_round)\n",
    "        # evaluate against test_this_round to produce the gold and predicted labels\n",
    "        goldlist = []\n",
    "        predictedlist = []\n",
    "        for (features, label) in test_this_round:\n",
    "            goldlist.append(label)\n",
    "            predictedlist.append(classifier.classify(features))\n",
    "\n",
    "        # computes evaluation measures for this fold and\n",
    "        #   returns list of measures for each label\n",
    "        print('Fold', i)\n",
    "        (precision_list, recall_list, F1_list) \\\n",
    "                  = eval_measures(goldlist, predictedlist, labels)\n",
    "        # take off triple string to print precision, recall and F1 for each fold\n",
    "        '''\n",
    "        print('\\tPrecision\\tRecall\\t\\tF1')\n",
    "        # print measures for each label\n",
    "        for i, lab in enumerate(labels):\n",
    "            print(lab, '\\t', \"{:10.3f}\".format(precision_list[i]), \\\n",
    "              \"{:10.3f}\".format(recall_list[i]), \"{:10.3f}\".format(F1_list[i]))\n",
    "        '''\n",
    "       \n",
    "        # for each label add to the sums in the total lists\n",
    "        for i in range(num_labels):\n",
    "            # for each label, add the 3 measures to the 3 lists of totals\n",
    "            total_precision_list[i] += precision_list[i]\n",
    "            total_recall_list[i] += recall_list[i]\n",
    "            total_F1_list[i] += F1_list[i]\n",
    "          \n",
    "  \n",
    "    # find precision, recall and F measure averaged over all rounds for all labels\n",
    "    # compute averages from the totals lists\n",
    "    precision_list = [tot/num_folds for tot in total_precision_list]\n",
    "    recall_list = [tot/num_folds for tot in total_recall_list]\n",
    "    F1_list = [tot/num_folds for tot in total_F1_list]\n",
    "    # the evaluation measures in a table with one row per label\n",
    "    print('\\nAverage Precision\\tRecall\\t\\tF1 \\tPer Label')\n",
    "    # print measures for each label\n",
    "    for i, lab in enumerate(labels):\n",
    "        print(lab, '\\t', \"{:10.3f}\".format(precision_list[i]), \\\n",
    "          \"{:10.3f}\".format(recall_list[i]), \"{:10.3f}\".format(F1_list[i]))\n",
    "    \n",
    "    # print macro average over all labels - treats each label equally\n",
    "    print('\\nMacro Average Precision\\tRecall\\t\\tF1 \\tOver All Labels')\n",
    "    print('\\t', \"{:10.3f}\".format(sum(precision_list)/num_labels), \\\n",
    "          \"{:10.3f}\".format(sum(recall_list)/num_labels), \\\n",
    "          \"{:10.3f}\".format(sum(F1_list)/num_labels))\n",
    "\n",
    "    # for micro averaging, weight the scores for each label by the number of items\n",
    "    #    this is better for labels with imbalance\n",
    "    # first intialize a dictionary for label counts and then count them\n",
    "    label_counts = {}\n",
    "    for lab in labels:\n",
    "      label_counts[lab] = 0 \n",
    "    # count the labels\n",
    "    for (doc, lab) in featuresets:\n",
    "      label_counts[lab] += 1\n",
    "    # make weights compared to the number of documents in featuresets\n",
    "    num_docs = len(featuresets)\n",
    "    label_weights = [(label_counts[lab] / num_docs) for lab in labels]\n",
    "    print('\\nLabel Counts', label_counts)\n",
    "    # print('Label weights', label_weights)\n",
    "    # print macro average over all labels\n",
    "    print('Micro Average Precision\\tRecall\\t\\tF1 \\tOver All Labels')\n",
    "    precision = sum([a * b for a,b in zip(precision_list, label_weights)])\n",
    "    recall = sum([a * b for a,b in zip(recall_list, label_weights)])\n",
    "    F1 = sum([a * b for a,b in zip(F1_list, label_weights)])\n",
    "    print( '\\t', \"{:10.3f}\".format(precision), \\\n",
    "      \"{:10.3f}\".format(recall), \"{:10.3f}\".format(F1))\n",
    "\n",
    "    #just to view the confusion matrix for the initial bag of words to show how the creating the bins was a beeter choice\n",
    "    # train_set, test_set = featuresets[round(.1*int(len(featuresets))):], featuresets[:round(.1*int(len(featuresets)))]\n",
    "    # classifier2 = nltk.NaiveBayesClassifier.train(train_set)\n",
    "    \n",
    "    # goldlist2 = []\n",
    "    # predictedlist2 = []\n",
    "\n",
    "    # for (features, label) in test_set:\n",
    "    #   goldlist2.append(label)\n",
    "    #   predictedlist2.append(classifier2.classify(features))\n",
    "\n",
    "    # print('\\nOverall Accuracy', nltk.classify.accuracy(classifier2, test_set))  \n",
    "    \n",
    "    # cm = nltk.ConfusionMatrix(goldlist2, predictedlist2)\n",
    "    \n",
    "    # print('\\nConfusion Matrix')\n",
    "    # print(cm.pretty_format(sort_by_count=True, show_percents=False, truncate = 9))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to compute precision, recall and F1 for each label\n",
    "#  and for any number of labels\n",
    "# Input: list of gold labels, list of predicted labels (in same order)\n",
    "# Output: returns lists of precision, recall and F1 for each label\n",
    "#      (for computing averages across folds and labels)\n",
    "def eval_measures(gold, predicted, labels):\n",
    "    \n",
    "    # these lists have values for each label \n",
    "    recall_list = []\n",
    "    precision_list = []\n",
    "    F1_list = []\n",
    "\n",
    "    for lab in labels:\n",
    "        # for each label, compare gold and predicted lists and compute values\n",
    "        TP = FP = FN = TN = 0\n",
    "        for i, val in enumerate(gold):\n",
    "            if val == lab and predicted[i] == lab:  TP += 1\n",
    "            if val == lab and predicted[i] != lab:  FN += 1\n",
    "            if val != lab and predicted[i] == lab:  FP += 1\n",
    "            if val != lab and predicted[i] != lab:  TN += 1\n",
    "        # use these to compute recall, precision, F1\n",
    "        # for small numbers, guard against dividing by zero in computing measures\n",
    "        if (TP == 0) or (FP == 0) or (FN == 0):\n",
    "          recall_list.append (0)\n",
    "          precision_list.append (0)\n",
    "          F1_list.append(0)\n",
    "        else:\n",
    "          recall = TP / (TP + FP)\n",
    "          precision = TP / (TP + FN)\n",
    "          recall_list.append(recall)\n",
    "          precision_list.append(precision)\n",
    "          F1_list.append( 2 * (recall * precision) / (recall + precision))\n",
    "\n",
    "    # the evaluation measures in a table with one row per label\n",
    "    return (precision_list, recall_list, F1_list)\n",
    "\n",
    "## function to read kaggle training file, train and test a classifier \n",
    "def processkaggle(dirPath,limitStr):\n",
    "  # convert the limit argument from a string to an int\n",
    "  limit = int(limitStr)\n",
    "  \n",
    "  dirPath0 = os.getcwd()\n",
    "  dirPath = dirPath0+'/corpus'\n",
    "  print('dirPath in processkaggle function: ', dirPath)\n",
    "  \n",
    "  f = open('./corpus/train.tsv', 'r')\n",
    "  # loop over lines in the file and use the first limit of them\n",
    "  phrasedata = []\n",
    "  for line in f:\n",
    "    # ignore the first line starting with Phrase and read all lines\n",
    "    if (not line.startswith('Phrase')):\n",
    "      # remove final end of line character\n",
    "      line = line.strip()\n",
    "      # each line has 4 items separated by tabs\n",
    "      # ignore the phrase and sentence ids, and keep the phrase and sentiment\n",
    "      phrasedata.append(line.split('\\t')[2:4])\n",
    "  \n",
    "  # pick a random sample of length limit because of phrase overlapping sequences\n",
    "  # Stack Overflow explanaiton of why to do it this way: https://stackoverflow.com/questions/19306976/python-shuffling-with-a-parameter-to-get-the-same-result\n",
    "\n",
    "  random.Random(1).shuffle(phrasedata)\n",
    "  phraselist = phrasedata[:limit]\n",
    "\n",
    "  print('Read', len(phrasedata), 'phrases, using', len(phraselist), 'random phrases')\n",
    "  \n",
    "  # create list of phrase documents as (list of words, label)\n",
    "  phrasedocs = []\n",
    "\n",
    "\n",
    "  # add all the phrases\n",
    "\n",
    "  # each phrase has a list of tokens and the sentiment label (from 0 to 4)\n",
    "  ### bin to only 3 categories for better performance\n",
    "  for phrase in phraselist:\n",
    "    tokens = nltk.word_tokenize(phrase[0])\n",
    "    sentiment = int(phrase[1])\n",
    "    \n",
    "    # remove triple quots and quote or comment out the new bins to see orignial\n",
    "    '''phrasedocs.append((tokens, int(phrase[1])))'''\n",
    "    \n",
    "    # create new bins\n",
    "    if (sentiment == 2):\n",
    "      phrasedocs.append((tokens, 'neutral'))\n",
    "    if ((sentiment == 0) or (sentiment == 1)):\n",
    "      phrasedocs.append((tokens, 'negative'))\n",
    "    if ((sentiment == 3) or (sentiment == 4)):\n",
    "      phrasedocs.append((tokens, 'positive')) \n",
    "    \n",
    "  # possibly filter tokens\n",
    "  # lowercase - each phrase is a pair consisting of a token list and a label\n",
    "  docs = []\n",
    "  for phrase in phrasedocs:\n",
    "    lowerphrase = ([w.lower() for w in phrase[0]], phrase[1])\n",
    "    docs.append (lowerphrase)\n",
    "  # print a few\n",
    "  for phrase in docs[:10]:\n",
    "    print (phrase)\n",
    "\n",
    "  # continue as usual to get all words and create word features\n",
    "  all_words_list = [word for (sent,cat) in docs for word in sent]\n",
    "  all_words = nltk.FreqDist(all_words_list)\n",
    "  print('\\nNumber of words: ',len(all_words))\n",
    "    \n",
    "\n",
    "  # get the 1500 most frequently appearing keywords in the corpus\n",
    "  word_items = all_words.most_common(1500)\n",
    "  word_features = [word for (word,count) in word_items]\n",
    "\n",
    "  stopwords = nltk.corpus.stopwords.words('english')\n",
    "  stopwords.extend([',', '.', '-', '``', '`', \"'\", \"...\", '--','movie','film'])\n",
    "\n",
    "\n",
    "  # remove some negation words \n",
    "  negationwords.extend(['ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn'])\n",
    "\n",
    "  newstopwords = [word for word in stopwords if word not in negationwords]\n",
    "  # remove stop words from the all words list\n",
    "  new_all_words_list = [word for (sent,cat) in docs for word in sent if word not in newstopwords]\n",
    "\n",
    "  # continue to define a new all words dictionary, get the 1500 most common as new_word_features\n",
    "  new_all_words = nltk.FreqDist(new_all_words_list)\n",
    "  new_word_items = new_all_words.most_common(1500)\n",
    "\n",
    "  new_word_features = [word for (word,count) in new_word_items]\n",
    "\n",
    "  #Creating Bigram features\n",
    "  finder = BigramCollocationFinder.from_words(all_words_list)\n",
    "  # define the top 500 bigrams using the chi squared measure\n",
    "  bigram_features = finder.nbest(bigram_measures.chi_sq, 500)\n",
    "\n",
    "  def bigram_document_features(document, word_features, bigram_features):\n",
    "      document_words = set(document)\n",
    "      document_bigrams = nltk.bigrams(document)\n",
    "      features = {}\n",
    "      for word in word_features:\n",
    "          features['V_{}'.format(word)] = (word in document_words)\n",
    "      for bigram in bigram_features:\n",
    "          features['B_{}_{}'.format(bigram[0], bigram[1])] = (bigram in document_bigrams)    \n",
    "      return features\n",
    "\n",
    "  # feature sets from feature definition functions above\n",
    "  featuresets = [(document_features(d, word_features), c) for (d, c) in docs]\n",
    "  stopfeaturesets = [(document_features(d, new_word_features), c) for (d, c) in docs]\n",
    "  negfeaturesets = [(NOT_features(d, word_features, negationwords), c) for (d, c) in docs]\n",
    "  bigramfeaturesets = [(bigram_document_features(d, word_features, bigram_features), c) for (d, c) in docs]\n",
    "  POSfeaturesets = [(POS_features(d, new_word_features), c) for (d, c) in docs]\n",
    "  # train classifier and show performance in cross-validation\n",
    "  # make a list of labels\n",
    "  label_list = [c for (d,c) in docs]\n",
    "  labels = list(set(label_list))    # gets only unique labels\n",
    "  num_folds = 10\n",
    "\n",
    "  #printing cross validation results for every feature and combination feature \n",
    "  #Unigram or Bag-of-Words (BOW) features; this is the baseline:\n",
    "  print(\"\\nOriginal Featureset\")\n",
    "  cross_validation_PRF(num_folds, featuresets, labels)\n",
    "  # Original w/ stopwords removed\n",
    "  print(\"\\nOriginal Featureset without stopwords\")\n",
    "  cross_validation_PRF(num_folds, stopfeaturesets, labels)\n",
    "  # Bigrams\n",
    "  print(\"\\nBigrams Featureset\")\n",
    "  cross_validation_PRF(num_folds, bigramfeaturesets, labels)\n",
    "  #Negation \n",
    "  print(\"\\nNegated Featureset\")\n",
    "  cross_validation_PRF(num_folds, negfeaturesets, labels)\n",
    "  #POS features\n",
    "  print(\"\\nPOS Featureset\")\n",
    "  cross_validation_PRF(num_folds, POSfeaturesets, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dirpath in program execution code (last line in the code):  c:\\Users\\mcfad\\Documents\\School\\IST NLP\\Final Project\\FinalProjectData\\kagglemoviereviews/corpus\n",
      "dirPath in processkaggle function:  c:\\Users\\mcfad\\Documents\\School\\IST NLP\\Final Project\\FinalProjectData\\kagglemoviereviews/corpus\n",
      "Read 156060 phrases, using 1000 random phrases\n",
      "(['through', 'horror', 'and', 'hellish', 'conditions'], 'negative')\n",
      "(['your', 'interest', ',', 'your', 'imagination', ',', 'your', 'empathy'], 'positive')\n",
      "(['it', \"'s\", 'a', 'crusty', 'treatment', 'of', 'a', 'clever', 'gimmick'], 'neutral')\n",
      "(['matinee', '.'], 'neutral')\n",
      "([\"'s\", 'the', 'days', 'of', 'our', 'lives'], 'neutral')\n",
      "(['her', 'dreams'], 'neutral')\n",
      "(['gets', 'us', 'in', 'trouble'], 'neutral')\n",
      "(['wear', 'down'], 'negative')\n",
      "(['a', 'film', 'of', 'epic', 'scale', 'with', 'an', 'intimate', 'feeling', ',', 'a', 'saga', 'of', 'the', 'ups', 'and', 'downs', 'of', 'friendships'], 'positive')\n",
      "(['disobedience'], 'neutral')\n",
      "\n",
      "Number of words:  2620\n",
      "\n",
      "Original Featureset\n",
      "\n",
      "Each fold size: 100\n",
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n",
      "Fold 6\n",
      "Fold 7\n",
      "Fold 8\n",
      "Fold 9\n",
      "\n",
      "Average Precision\tRecall\t\tF1 \tPer Label\n",
      "neutral \t      0.866      0.576      0.690\n",
      "positive \t      0.234      0.496      0.314\n",
      "negative \t      0.258      0.466      0.327\n",
      "\n",
      "Macro Average Precision\tRecall\t\tF1 \tOver All Labels\n",
      "\t      0.453      0.513      0.444\n",
      "\n",
      "Label Counts {'neutral': 494, 'positive': 270, 'negative': 236}\n",
      "Micro Average Precision\tRecall\t\tF1 \tOver All Labels\n",
      "\t      0.552      0.529      0.503\n",
      "\n",
      "Original Featureset without stopwords\n",
      "\n",
      "Each fold size: 100\n",
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n",
      "Fold 6\n",
      "Fold 7\n",
      "Fold 8\n",
      "Fold 9\n",
      "\n",
      "Average Precision\tRecall\t\tF1 \tPer Label\n",
      "neutral \t      0.877      0.553      0.676\n",
      "positive \t      0.200      0.476      0.275\n",
      "negative \t      0.166      0.413      0.232\n",
      "\n",
      "Macro Average Precision\tRecall\t\tF1 \tOver All Labels\n",
      "\t      0.414      0.480      0.394\n",
      "\n",
      "Label Counts {'neutral': 494, 'positive': 270, 'negative': 236}\n",
      "Micro Average Precision\tRecall\t\tF1 \tOver All Labels\n",
      "\t      0.527      0.499      0.463\n",
      "\n",
      "Bigrams Featureset\n",
      "\n",
      "Each fold size: 100\n",
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n",
      "Fold 6\n",
      "Fold 7\n",
      "Fold 8\n",
      "Fold 9\n",
      "\n",
      "Average Precision\tRecall\t\tF1 \tPer Label\n",
      "neutral \t      0.866      0.577      0.691\n",
      "positive \t      0.234      0.494      0.314\n",
      "negative \t      0.258      0.466      0.327\n",
      "\n",
      "Macro Average Precision\tRecall\t\tF1 \tOver All Labels\n",
      "\t      0.453      0.512      0.444\n",
      "\n",
      "Label Counts {'neutral': 494, 'positive': 270, 'negative': 236}\n",
      "Micro Average Precision\tRecall\t\tF1 \tOver All Labels\n",
      "\t      0.552      0.529      0.503\n",
      "\n",
      "Negated Featureset\n",
      "\n",
      "Each fold size: 100\n",
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n",
      "Fold 6\n",
      "Fold 7\n",
      "Fold 8\n",
      "Fold 9\n",
      "\n",
      "Average Precision\tRecall\t\tF1 \tPer Label\n",
      "neutral \t      0.866      0.575      0.689\n",
      "positive \t      0.240      0.490      0.319\n",
      "negative \t      0.237      0.449      0.304\n",
      "\n",
      "Macro Average Precision\tRecall\t\tF1 \tOver All Labels\n",
      "\t      0.448      0.505      0.438\n",
      "\n",
      "Label Counts {'neutral': 494, 'positive': 270, 'negative': 236}\n",
      "Micro Average Precision\tRecall\t\tF1 \tOver All Labels\n",
      "\t      0.549      0.522      0.498\n",
      "\n",
      "POS Featureset\n",
      "\n",
      "Each fold size: 100\n",
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n",
      "Fold 6\n",
      "Fold 7\n",
      "Fold 8\n",
      "Fold 9\n",
      "\n",
      "Average Precision\tRecall\t\tF1 \tPer Label\n",
      "neutral \t      0.866      0.583      0.695\n",
      "positive \t      0.223      0.464      0.299\n",
      "negative \t      0.268      0.472      0.338\n",
      "\n",
      "Macro Average Precision\tRecall\t\tF1 \tOver All Labels\n",
      "\t      0.452      0.506      0.444\n",
      "\n",
      "Label Counts {'neutral': 494, 'positive': 270, 'negative': 236}\n",
      "Micro Average Precision\tRecall\t\tF1 \tOver All Labels\n",
      "\t      0.551      0.525      0.504\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "commandline interface takes a directory name with kaggle subdirectory for train.tsv\n",
    "   and a limit to the number of kaggle phrases to use\n",
    "It then processes the files and trains a kaggle movie review sentiment classifier.\n",
    "\n",
    "\"\"\"\n",
    "dirPath0 = os.getcwd()\n",
    "dirPath = dirPath0+'/corpus'\n",
    "print('dirpath in program execution code (last line in the code): ', dirPath)\n",
    "processkaggle(dirPath, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.11 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "9446dff6ce575b99a2aeee30062ed671b3efb334d0b92e66b4fe7e659d79278c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
